{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "efdfa33f",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## load path\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "sys.path.append('../models')\n",
    "from pathlib import Path\n",
    "\n",
    "## load utils \n",
    "from util.util import *\n",
    "import time\n",
    "import scipy.io as scio\n",
    "from tqdm import tqdm\n",
    "from einops import rearrange\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# load PDE utils\n",
    "from pde_utils.testloss import TestLoss\n",
    "from pde_utils.normalizer import UnitTransformer\n",
    "from pde_utils.pde_loader import *\n",
    "\n",
    "## load modules \n",
    "from models.mino_transformer import MINO\n",
    "from models.mino_modules.decoder_perceiver import DecoderPerceiver\n",
    "from models.mino_modules.encoder_supernodes_gno_cross_attention import EncoderSupernodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8779b4f-8a9d-4a72-9226-d79b77aa450e",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5570f3d2-e13a-4d6e-9e04-4cb45fb08f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "dims = [85, 85]\n",
    "query_dims = [16, 16]\n",
    "x_dim = 2\n",
    "\n",
    "device = 'cuda:1' if torch.cuda.is_available() else 'cpu'\n",
    "spath = Path('/net/ghisallo/scratch1/yshi5/OFM_PDE/MINO/MINO_T_Darcy_PDE')\n",
    "\n",
    "spath.mkdir(parents=True, exist_ok=True)\n",
    "saved_model = True # save model\n",
    "save_int = 500\n",
    "\n",
    "# model hyperparameters\n",
    "## conditional time step.. \n",
    "dim = 256\n",
    "num_heads=4\n",
    "\n",
    "## training parameters\n",
    "epochs = 500\n",
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f3c9b5-deac-4a83-998e-e304b600f71a",
   "metadata": {},
   "source": [
    "### Dataset and Dataloder\n",
    "\n",
    "`x_train` : Input function, contains N samples, with shape (N, n_chan, n_seq), where `n_seq` is the number of discretizations\n",
    "\n",
    "`y_train` : output function, with shape (N, n_chan_2, n_seq)\n",
    "\n",
    "`pos_data` : (N, x_dim, n_seq), `x_dim` is the dimension for the domain, for 2D `x_dim` = 2\n",
    "\n",
    "`query_pos` : inquired position for GNO, has a shape of (x_dim, n_node). n_node << n_seq. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdf0353a-32b6-4c25-80da-ee625cf9fe91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datapath \n",
    "data_path = Path('/net/ghisallo/scratch1/yshi5/OFM_PDE/MINO/PDE_datasets/Darcy_421')\n",
    "train_path = data_path /'piececonst_r421_N1024_smooth1.mat'\n",
    "test_path = data_path /'piececonst_r421_N1024_smooth2.mat' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4def78ea-8c4f-48ac-aed9-f95ceb04d266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataloading is over.\n"
     ]
    }
   ],
   "source": [
    "#### We follow the pipeline of Transolver for processing the Darcy flow data\n",
    "## the resolution is [85, 85]\n",
    "\n",
    "ntrain = 1000\n",
    "ntest = 200\n",
    "downsample = 5\n",
    "\n",
    "r = downsample\n",
    "h = int(((421 - 1) / r) + 1) #85\n",
    "s = h\n",
    "dx = 1.0 / s\n",
    "\n",
    "train_data = scio.loadmat(train_path)\n",
    "x_train = train_data['coeff'][:ntrain, ::r, ::r][:, :s, :s]\n",
    "x_train = torch.from_numpy(x_train).float()\n",
    "x_train = torch.flatten(x_train.unsqueeze(1), start_dim=2)\n",
    "\n",
    "y_train = train_data['sol'][:ntrain, ::r, ::r][:, :s, :s]\n",
    "y_train = torch.from_numpy(y_train)\n",
    "y_train = torch.flatten(y_train.unsqueeze(1), start_dim=2)\n",
    "\n",
    "test_data = scio.loadmat(test_path)\n",
    "x_test = test_data['coeff'][:ntest, ::r, ::r][:, :s, :s]\n",
    "x_test = torch.from_numpy(x_test).float()\n",
    "x_test = torch.flatten(x_test.unsqueeze(1), start_dim=2)\n",
    "\n",
    "y_test = test_data['sol'][:ntest, ::r, ::r][:, :s, :s]\n",
    "y_test = torch.from_numpy(y_test)\n",
    "y_test = torch.flatten(y_test.unsqueeze(1), start_dim=2)\n",
    "\n",
    "x_normalizer = UnitTransformer(x_train)\n",
    "y_normalizer = UnitTransformer(y_train)\n",
    "\n",
    "x_train = x_normalizer.encode(x_train)\n",
    "x_test = x_normalizer.encode(x_test)\n",
    "y_train = y_normalizer.encode(y_train)\n",
    "\n",
    "# y_test is not encoded\n",
    "\n",
    "x_normalizer.to(device)\n",
    "y_normalizer.to(device)\n",
    "\n",
    "x = np.linspace(0, 1, s)\n",
    "y = np.linspace(0, 1, s)\n",
    "x, y = np.meshgrid(x, y)\n",
    "pos = np.c_[x.ravel(), y.ravel()]\n",
    "pos = torch.tensor(pos, dtype=torch.float).unsqueeze(0)\n",
    "pos = pos.permute(0, 2, 1)\n",
    "\n",
    "pos_train = pos.repeat(ntrain, 1, 1)\n",
    "pos_test = pos.repeat(ntest, 1, 1)\n",
    "\n",
    "## latent position \n",
    "query_pos = make_2d_grid(query_dims).permute(1,0) #[2, 16x16]\n",
    "\n",
    "train_dataset = SimDataset_PDE(input_data=x_train, output_data=y_train, pos=pos_train, query_pos=query_pos)\n",
    "test_dataset = SimDataset_PDE(input_data=x_test, output_data=y_test, pos=pos_test, query_pos=query_pos)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=SimulationCollator_PDE,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=SimulationCollator_PDE,\n",
    ")\n",
    "print(\"Dataloading is over.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677f39f4",
   "metadata": {},
   "source": [
    "## Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de074b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters: 6.5M\n"
     ]
    }
   ],
   "source": [
    "model = MINO(\n",
    "    encoder=EncoderSupernodes(\n",
    "        input_dim=1, # co-domain \n",
    "        ndim=2, # dimension of domain\n",
    "        radius= 0.07,\n",
    "        enc_dim=dim,\n",
    "        enc_num_heads=num_heads,\n",
    "        enc_depth=5,\n",
    "    ),\n",
    "    decoder=DecoderPerceiver(\n",
    "        input_dim=dim,\n",
    "        output_dim=1,\n",
    "        ndim=2,\n",
    "        dim=dim,\n",
    "        num_heads=num_heads,\n",
    "        depth=2, # 2 layers\n",
    "        unbatch_mode=\"dense_to_sparse_unpadded\",\n",
    "    ),\n",
    ")\n",
    "model = model.to(device)\n",
    "print(f\"parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.1f}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a31c385-21d5-42db-9d2b-0518b6fd199b",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.8)\n",
    "\n",
    "myloss = TestLoss(size_average=False)\n",
    "de_x = TestLoss(size_average=False)\n",
    "de_y = TestLoss(size_average=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f90b64-99f7-410b-852d-ade133b25f73",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2ad67b1-94c1-40ea-92e6-a1cf9b6881bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def central_diff(x: torch.Tensor, h, resolution):\n",
    "    # assuming PBC\n",
    "    # x: (batch, n, channel), h is the step size, assuming n = h*w\n",
    "    # x : (batch, n_cahnel, n_seq)\n",
    "    x = rearrange(x, 'b c (h w) -> b h w c', h=resolution, w=resolution)\n",
    "    #x = rearrange(x, 'b (h w) c -> b h w c', h=resolution, w=resolution)\n",
    "    x = F.pad(x,\n",
    "              (0, 0, 1, 1, 1, 1), mode='constant', value=0.)  # [b c t h+2 w+2]\n",
    "    grad_x = (x[:, 1:-1, 2:, :] - x[:, 1:-1, :-2, :]) / (2 * h)  # f(x+h) - f(x-h) / 2h\n",
    "    grad_y = (x[:, 2:, 1:-1, :] - x[:, :-2, 1:-1, :]) / (2 * h)  # f(x+h) - f(x-h) / 2h\n",
    "\n",
    "    return grad_x, grad_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d708fea-6111-4fd1-88cc-3d4bfa6fc90b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9a2308-9b8d-49c9-b46e-17216769bb6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Reg : 0.00000 Train loss : 0.20346\n",
      "rel_err:4.999703094507945\n",
      "Epoch 2 Reg : 0.00000 Train loss : 0.20072\n",
      "rel_err:4.999638598209597\n",
      "Epoch 3 Reg : 0.00000 Train loss : 0.19983\n",
      "rel_err:4.9996180349893695\n",
      "Epoch 4 Reg : 0.00000 Train loss : 0.19955\n",
      "rel_err:4.999611495814756\n"
     ]
    }
   ],
   "source": [
    "for ep in range(1, epochs+1):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    reg = 0\n",
    "    test_losses = []\n",
    "    for batch_pack in train_loader:\n",
    "        batch = batch_pack['input_feat'].to(device) # [batch_size, n_chan, n_seq]\n",
    "        pos = batch_pack['input_pos'].to(device) # [batch_size, x_dim, n_seq]\n",
    "        query_pos = batch_pack['query_pos'].to(device)\n",
    "        out_batch = batch_pack['output_feat'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        out = model(input_feat=batch, input_pos=pos, query_pos=query_pos)\n",
    "        out = y_normalizer.decode(out) # 'b, c, (h w)'\n",
    "        out_batch = y_normalizer.decode(out_batch) # 'b, c, (h w)'\n",
    "\n",
    "        l2loss = myloss(out, out_batch)\n",
    "        \n",
    "        # derivative loss for Darcy Flow dataset\n",
    "        \"\"\"\n",
    "        out = rearrange(out, 'b c (h w) -> b c h w', h=s)\n",
    "        out = out[..., 1:-1, 1:-1].contiguous()\n",
    "        out = F.pad(out, (1, 1, 1, 1), \"constant\", 0)\n",
    "        out = rearrange(out, 'b c h w -> b c (h w)')\n",
    "        gt_grad_x, gt_grad_y = central_diff(out_batch, dx, s)\n",
    "        pred_grad_x, pred_grad_y = central_diff(out, dx, s)\n",
    "        deriv_loss = de_x(pred_grad_x, gt_grad_x) + de_y(pred_grad_y, gt_grad_y)\n",
    "        loss = 0.1 * deriv_loss + l2loss\n",
    "        loss.backward()   \n",
    "        \"\"\"\n",
    "        l2loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        train_loss += l2loss.item()\n",
    "        #reg += deriv_loss.item()\n",
    "        scheduler.step()   \n",
    "        \n",
    "    train_loss /= ntrain\n",
    "    reg /= ntrain\n",
    "    print(\"Epoch {} Reg : {:.5f} Train loss : {:.5f}\".format(ep, reg, train_loss))\n",
    "    \n",
    "    model.eval()\n",
    "    rel_err = 0.0\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for batch_pack in train_loader:\n",
    "            batch = batch_pack['input_feat'].to(device) # [batch_size, n_chan, n_seq]\n",
    "            pos = batch_pack['input_pos'].to(device) # [batch_size, x_dim, n_seq]\n",
    "            query_pos = batch_pack['query_pos'].to(device)\n",
    "            out_batch = batch_pack['output_feat'].to(device)\n",
    "            \n",
    "            out = model(input_feat=batch, input_pos=pos, query_pos=query_pos)\n",
    "            out = y_normalizer.decode(out) # 'b, c, (h w)'\n",
    "    \n",
    "            tl = myloss(out, out_batch).item()            \n",
    "            rel_err += tl\n",
    "\n",
    "    rel_err /= ntest\n",
    "    print(\"rel_err:{}\".format(rel_err))\n",
    "\n",
    "    test_losses.append(rel_err)\n",
    "    ##### BOOKKEEPING\n",
    "    if saved_model == True:\n",
    "        if ep % save_int == 0:\n",
    "            torch.save(model.state_dict(), save_path / f'epoch_{ep}.pt')\n",
    "            np.save(save_path / 'test_loss_epoch.npy', np.array(test_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782d1db3-8cb9-4e17-a7a6-0157a926bc5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8ef40deb",
   "metadata": {},
   "source": [
    "## Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1abc2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the trained model\n",
    "spath = Path('/net/ghisallo/scratch1/yshi5/OFM_PDE/GITO_exp/GITO_T_NS')\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "model_path = os.path.join(spath, 'epoch_500.pt')\n",
    "checkpoint = torch.load(model_path, map_location='cpu', weights_only=False)\n",
    "model.load_state_dict(checkpoint, strict=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4067f1e4-aa7a-4f92-ab0a-d9678d9875f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_pde",
   "language": "python",
   "name": "test_pde"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efdfa33f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## load path\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "sys.path.append('../models')\n",
    "from pathlib import Path\n",
    "\n",
    "## load utils \n",
    "from util.util import *\n",
    "from util.true_gaussian_process_seq import *\n",
    "from util.ofm_OT_likelihood_seq_mino import *\n",
    "from util.metrics import *\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import binned_statistic\n",
    "\n",
    "## load modules \n",
    "from models.mino_unet import MINO\n",
    "from models.mino_modules.decoder_perceiver import DecoderPerceiver\n",
    "from models.mino_modules.encoder_supernodes_gno_cross_attention import EncoderSupernodes\n",
    "from models.mino_modules.conditioner_timestep import ConditionerTimestep\n",
    "\n",
    "from models.mino_modules.modules.unet_nD import UNetModelWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "277e7183",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_x = 64\n",
    "dims = [n_x, n_x]\n",
    "query_dims = [n_x//4, n_x//4]\n",
    "x_dim = 2\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "spath = Path('./saved_models/MINO_U_NS')\n",
    "\n",
    "spath.mkdir(parents=True, exist_ok=True)\n",
    "saved_model = True # save model\n",
    "\n",
    "# GP hyperparameters\n",
    "kernel_length=0.01\n",
    "kernel_variance=1\n",
    "nu = 0.5 # default\n",
    "\n",
    "# model hyperparameters\n",
    "## conditional time step.. \n",
    "dim = 256\n",
    "num_heads=4\n",
    "\n",
    "## u-net parameters\n",
    "unet_dims = (dim, 16, 16)\n",
    "unet_channels = 64\n",
    "num_res_blocks=1\n",
    "num_unet_heads=4\n",
    "attention_res = '8'\n",
    "\n",
    "## training parameters\n",
    "epochs = 300\n",
    "sigma_min=1e-4 \n",
    "batch_size = 96"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e83b2e",
   "metadata": {},
   "source": [
    "### Dataset and Dataloder\n",
    "\n",
    "`x_train` : contains N samples, with shape (N, n_chan, n_seq), where `n_seq` is the number of discretizations\n",
    "\n",
    "`pos_data` : (N, x_dim, n_seq), `x_dim` is the dimension for the domain, for 2D `x_dim` = 2\n",
    "\n",
    "`query_pos` : inquired position for GNO, has a shape of (x_dim, n_node). n_node << n_seq. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72cee52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.load('../dataset/N_S/ns_30000.npy')\n",
    "x_train = torch.flatten(torch.Tensor(x_train), start_dim=2)\n",
    "\n",
    "n_pos = make_2d_grid(dims) #64x64 \n",
    "pos_data = n_pos.unsqueeze(0).repeat(len(x_train), 1, 1).permute(0,2,1) \n",
    "\n",
    "query_pos = make_2d_grid(query_dims).permute(1,0) #[2, 16x16]\n",
    "train_dataset = SimDataset(x_train, pos_data, query_pos)\n",
    "\n",
    "loader_tr =  DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=SimulationCollator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677f39f4",
   "metadata": {},
   "source": [
    "## Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de074b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "conditioner = ConditionerTimestep(\n",
    "    dim=dim\n",
    ")\n",
    "model = MINO(\n",
    "    conditioner=conditioner,\n",
    "    encoder=EncoderSupernodes(\n",
    "        input_dim=1, # co-domain \n",
    "        ndim=2, # dimension of domain\n",
    "        radius= 0.07,\n",
    "        enc_dim=dim,\n",
    "        enc_num_heads=num_heads,\n",
    "        enc_depth=2,\n",
    "        cond_dim=conditioner.cond_dim,\n",
    "    ),\n",
    "    \n",
    "    processor=UNetModelWrapper(dim=unet_dims, num_channels=unet_channels,\n",
    "                                          num_res_blocks=num_res_blocks,\n",
    "                                          num_heads=num_unet_heads, set_cond=False,\n",
    "                                          attention_resolutions=attention_res),\n",
    "    \n",
    "    decoder=DecoderPerceiver(\n",
    "        input_dim=dim,\n",
    "        output_dim=1,\n",
    "        ndim=2,\n",
    "        dim=dim,\n",
    "        num_heads=num_heads,\n",
    "        depth=2, # 2 layers\n",
    "        unbatch_mode=\"dense_to_sparse_unpadded\",\n",
    "        cond_dim=conditioner.cond_dim,\n",
    "    ),\n",
    ")\n",
    "model = model.to(device)\n",
    "#print(f\"parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.1f}M\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76f44ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=25, gamma=0.8)\n",
    "fmot = OFMModel(model, kernel_length=kernel_length, kernel_variance=kernel_variance, nu=nu, sigma_min=sigma_min, device=device, x_dim=x_dim, n_pos=n_pos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1cd9123",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fmot.train(loader_tr, optimizer, epochs=epochs, scheduler=scheduler, eval_int=int(0), save_int=int(300), generate=False, save_path=spath,saved_model=saved_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52665f9",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4525e641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the trained model\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "model_path = os.path.join(spath, 'epoch_300.pt')\n",
    "checkpoint = torch.load(model_path, map_location='cpu', weights_only=False)\n",
    "model.load_state_dict(checkpoint, strict=False)\n",
    "fmot = OFMModel(model, kernel_length=kernel_length, kernel_variance=kernel_variance, nu=nu, sigma_min=sigma_min, device=device, x_dim=x_dim, n_pos=n_pos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d0beb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test dataset \n",
    "x_test = np.load('../dataset/N_S/ns_test_10000.npy')\n",
    "x_test = torch.Tensor(x_test)\n",
    "x_test = x_test[:5000] # keep first 5000 test sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99bf6fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_meta_info(batch_size, dims, query_dims):\n",
    "\n",
    "    n_pos = make_2d_grid(dims)  \n",
    "    pos_data = n_pos.unsqueeze(0).repeat(batch_size, 1, 1)\n",
    "    \n",
    "    query_n_pos = make_2d_grid(query_dims) # unchanged\n",
    "    query_pos_data = query_n_pos.unsqueeze(0).repeat(batch_size, 1, 1)\n",
    "\n",
    "    collated_batch = {}\n",
    "\n",
    "    collated_batch[\"input_pos\"] = pos_data.permute(0, 2, 1)\n",
    "    collated_batch['query_pos']= query_pos_data.permute(0, 2, 1)\n",
    "    \n",
    "    return collated_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb874fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82acae0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "\n",
    "\n",
    "    collated_batch =  gen_meta_info(batch_size=10, dims=dims, query_dims=query_dims)\n",
    "    pos, query_pos = collated_batch['input_pos'], collated_batch['query_pos']\n",
    "    X_hat = fmot.sample(pos=pos.to(device), query_pos=query_pos.to(device), n_samples=10, n_eval=10).cpu()\n",
    "    X_hat = X_hat.reshape(X_hat.shape[0], *dims)\n",
    "    \n",
    "    X_ground_truth = x_test[:5].squeeze()\n",
    "    # plot\n",
    "    \n",
    "    X_alt = []\n",
    "    # generate 5000 synthetic samples\n",
    "    for i in range(25):\n",
    "        collated_batch =  gen_meta_info(batch_size=200, dims=dims, query_dims=query_dims)\n",
    "        pos, query_pos = collated_batch['input_pos'], collated_batch['query_pos']\n",
    "        X_temp = fmot.sample(pos=pos.to(device), query_pos=query_pos.to(device), n_samples=200, n_eval=2).cpu()\n",
    "    \n",
    "        X_alt.append(X_temp)\n",
    "        \n",
    "    X_alt = torch.vstack(X_alt).squeeze()\n",
    "    \n",
    "    X_alt = X_alt.reshape(X_alt.shape[0], *dims)\n",
    "\n",
    "    \n",
    "    bin_center, x_acovf = compute_acovf(X_alt.squeeze())\n",
    "    _, x_acovf_true = compute_acovf(x_test.squeeze())\n",
    "    x_hist, bin_edges_alt = X_alt.histogram(range=[-4,4], density=True)\n",
    "    x_hist_true, bin_edges = x_test.histogram(range=[-4, 4], density=True)\n",
    "    \n",
    "    fig, ax = plt.subplots(1,5, figsize=(15,3))\n",
    "    for i in range(5):\n",
    "        x = X_hat[i,:,:].squeeze()\n",
    "\n",
    "        ax[i].imshow(x,cmap=\"RdBu_r\")#, vmin=-2, vmax=2)\n",
    "        if i == 0:\n",
    "            ax[i].set_ylabel('OFM', fontsize=16)\n",
    "        \n",
    "    \n",
    "    fig, ax = plt.subplots(1,5, figsize=(15,3))    \n",
    "    for i in range(5):\n",
    "        x_ground_truth = X_ground_truth[i,:,:].squeeze()\n",
    "        ax[i].imshow(x_ground_truth,cmap=\"RdBu_r\")#, vmin=-2, vmax=2)\n",
    "        if i == 0:\n",
    "            ax[i].set_ylabel('Ground Truth', fontsize=16)\n",
    "\n",
    "   \n",
    "    fig, ax = plt.subplots(1,1, figsize=(6,4))\n",
    "    ax.plot(bin_center, x_acovf_true, c='k', lw=3, label='Ground Truth')\n",
    "    ax.plot(bin_center, x_acovf, c='r',ls='--', lw=3, label='OFM')\n",
    "    ax.set_title('Autocovariance')\n",
    "    ax.set_xlabel('Number of lags')\n",
    "    ax.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ebe20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7864453d",
   "metadata": {},
   "source": [
    "## metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2032516e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For fluid\n",
    "hist_mse = torch.mean((x_hist_true - x_hist)**2)\n",
    "\n",
    "cov_mse = np.mean((x_acovf_true[~np.isnan(x_acovf_true)] - x_acovf[~np.isnan(x_acovf)])**2)\n",
    "\n",
    "true_spect = spectrum_2d(torch.Tensor(x_test), 64)\n",
    "spect = spectrum_2d(X_alt, 64)\n",
    "spect_mse = torch.mean((true_spect - spect)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258e186c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## general metric \n",
    "swd_value = swd_stable(X=X_alt, Y=x_test)\n",
    "mmd_value = unbiased_mmd2_torch(X=X_alt, Y=x_test, device=device)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa77b3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('hist_mes:{},  \\ncov_mse:{}, \\nspect_mse:{}, \\nswd:{:.2f}, mmd:{:.2f}'.format(hist_mse, cov_mse, spect_mse, swd_value, mmd_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d49f94",
   "metadata": {},
   "source": [
    "## Zero-shot super-resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22b2a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "dims_sup = [128, 128]\n",
    "n_pos_sup = make_2d_grid(dims_sup)\n",
    "pos_data_sup = n_pos_sup.unsqueeze(0).repeat(10, 1, 1).permute(0,2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a3134f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "\n",
    "    collated_batch =  gen_meta_info(batch_size=10, dims=dims_sup, query_dims=query_dims)\n",
    "    pos, query_pos = collated_batch['input_pos'], collated_batch['query_pos'] # latent query is unchanged\n",
    "    X_hat = fmot.sample(pos=pos.to(device), query_pos=query_pos.to(device), n_samples=10, n_eval=10).cpu()\n",
    "    X_hat = X_hat.reshape(X_hat.shape[0], *dims_sup)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b7ee3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "\n",
    "    fig, ax = plt.subplots(1,5, figsize=(15,3))\n",
    "    for i in range(5):\n",
    "        x = X_hat[i,:,:].squeeze()\n",
    "\n",
    "        ax[i].imshow(x,cmap=\"RdBu_r\")#, vmin=-2, vmax=2)\n",
    "        if i == 0:\n",
    "            ax[i].set_ylabel('OFM-Super resolution', fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7d0677",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

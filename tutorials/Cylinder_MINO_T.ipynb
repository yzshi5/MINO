{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efdfa33f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## load path\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "sys.path.append('../models')\n",
    "from pathlib import Path\n",
    "\n",
    "## load utils \n",
    "from util.util import *\n",
    "\n",
    "# we provide two realizations of GP on sphere, the commented one is slower\n",
    "#from util.ofm_OT_likelihood_sphere_seq_mino import * \n",
    "from util.ofm_OT_likelihood_seq_mino import *\n",
    "from util.metrics import *\n",
    "import time\n",
    "\n",
    "## load modules \n",
    "from models.mino_transformer import MINO\n",
    "from models.mino_modules.decoder_perceiver import DecoderPerceiver\n",
    "from models.mino_modules.encoder_supernodes_gno_cross_attention import EncoderSupernodes\n",
    "from models.mino_modules.conditioner_timestep import ConditionerTimestep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "277e7183",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_dims = [16, 16]\n",
    "x_dim = 2\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "spath = Path('./saved_models/MINO_T_Cylinder')\n",
    "\n",
    "spath.mkdir(parents=True, exist_ok=True)\n",
    "saved_model = True # save model\n",
    "\n",
    "# GP hyperparameters\n",
    "kernel_length=0.01\n",
    "kernel_variance=1\n",
    "nu = 0.5 # default\n",
    "\n",
    "# model hyperparameters\n",
    "## conditional time step.. \n",
    "dim = 256\n",
    "num_heads=4\n",
    "\n",
    "## training parameters\n",
    "epochs = 300\n",
    "sigma_min=1e-4 \n",
    "batch_size = 96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "154803b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [batch_size, n_chan, n_seq]\n",
    "# pos_data [bathc_size, x_dim, n_seq]\n",
    "x_train = np.load('../dataset/cylinder/x_train.npy')\n",
    "x_train = torch.Tensor(x_train).permute(0, 2, 1)\n",
    "\n",
    "n_pos = np.load('../dataset/cylinder/pos_normalized.npy')\n",
    "n_pos = torch.Tensor(n_pos)\n",
    "pos_data = n_pos.unsqueeze(0).repeat(len(x_train), 1, 1).permute(0, 2, 1)\n",
    "\n",
    "query_pos = make_2d_grid(query_dims).permute(1,0) #[2, 16x16]\n",
    "train_dataset = SimDataset(x_train, pos_data, query_pos)\n",
    "\n",
    "loader_tr =  DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=SimulationCollator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677f39f4",
   "metadata": {},
   "source": [
    "## Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "586b81b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size -> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de074b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "conditioner = ConditionerTimestep(\n",
    "    dim=dim\n",
    ")\n",
    "model = MINO(\n",
    "    conditioner=conditioner,\n",
    "    encoder=EncoderSupernodes(\n",
    "        input_dim=3, # co-domain \n",
    "        ndim=2, # dimension of domain\n",
    "        radius= 0.07,\n",
    "        enc_dim=dim,\n",
    "        enc_num_heads=num_heads,\n",
    "        enc_depth=5,\n",
    "        cond_dim=conditioner.cond_dim,\n",
    "    ),\n",
    "    decoder=DecoderPerceiver(\n",
    "        input_dim=dim,\n",
    "        output_dim=3,\n",
    "        ndim=2,\n",
    "        dim=dim,\n",
    "        num_heads=num_heads,\n",
    "        depth=2, # 2 layers\n",
    "        unbatch_mode=\"dense_to_sparse_unpadded\",\n",
    "        cond_dim=conditioner.cond_dim,\n",
    "    ),\n",
    ")\n",
    "model = model.to(device)\n",
    "#print(f\"parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.1f}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76f44ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=25, gamma=0.8)\n",
    "fmot = OFMModel(model, kernel_length=kernel_length, kernel_variance=kernel_variance, nu=nu, sigma_min=sigma_min, device=device, x_dim=x_dim, n_pos=n_pos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e1cd9123",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tr @ epoch 1/300 | Loss 0.705979 | 149.53 (s)\n",
      "tr @ epoch 2/300 | Loss 0.439543 | 152.90 (s)\n",
      "tr @ epoch 3/300 | Loss 0.275217 | 153.47 (s)\n",
      "tr @ epoch 4/300 | Loss 0.231169 | 153.75 (s)\n",
      "tr @ epoch 5/300 | Loss 0.209339 | 153.79 (s)\n",
      "tr @ epoch 6/300 | Loss 0.189943 | 153.80 (s)\n",
      "tr @ epoch 7/300 | Loss 0.164093 | 153.92 (s)\n",
      "tr @ epoch 8/300 | Loss 0.136082 | 154.02 (s)\n",
      "tr @ epoch 9/300 | Loss 0.120323 | 154.04 (s)\n",
      "tr @ epoch 10/300 | Loss 0.111568 | 154.08 (s)\n",
      "tr @ epoch 11/300 | Loss 0.105005 | 154.13 (s)\n",
      "tr @ epoch 12/300 | Loss 0.100770 | 154.27 (s)\n",
      "tr @ epoch 13/300 | Loss 0.096420 | 154.29 (s)\n",
      "tr @ epoch 14/300 | Loss 0.091613 | 154.22 (s)\n",
      "tr @ epoch 15/300 | Loss 0.089617 | 154.35 (s)\n",
      "tr @ epoch 16/300 | Loss 0.087622 | 154.27 (s)\n",
      "tr @ epoch 17/300 | Loss 0.084136 | 154.21 (s)\n",
      "tr @ epoch 18/300 | Loss 0.083936 | 154.13 (s)\n",
      "tr @ epoch 19/300 | Loss 0.081392 | 154.07 (s)\n",
      "tr @ epoch 20/300 | Loss 0.081543 | 153.83 (s)\n",
      "tr @ epoch 21/300 | Loss 0.076952 | 153.73 (s)\n",
      "tr @ epoch 22/300 | Loss 0.077090 | 153.75 (s)\n",
      "tr @ epoch 23/300 | Loss 0.074934 | 153.72 (s)\n",
      "tr @ epoch 24/300 | Loss 0.073208 | 153.75 (s)\n",
      "tr @ epoch 25/300 | Loss 0.074387 | 153.74 (s)\n",
      "tr @ epoch 26/300 | Loss 0.065448 | 153.71 (s)\n",
      "tr @ epoch 27/300 | Loss 0.066882 | 153.74 (s)\n",
      "tr @ epoch 28/300 | Loss 0.064097 | 153.94 (s)\n",
      "tr @ epoch 29/300 | Loss 0.065396 | 154.02 (s)\n",
      "tr @ epoch 30/300 | Loss 0.068125 | 154.21 (s)\n",
      "tr @ epoch 31/300 | Loss 0.062252 | 154.26 (s)\n",
      "tr @ epoch 32/300 | Loss 0.062772 | 154.33 (s)\n",
      "tr @ epoch 33/300 | Loss 0.061074 | 154.28 (s)\n",
      "tr @ epoch 34/300 | Loss 0.062750 | 154.23 (s)\n",
      "tr @ epoch 35/300 | Loss 0.061718 | 154.16 (s)\n",
      "tr @ epoch 36/300 | Loss 0.061466 | 154.21 (s)\n",
      "tr @ epoch 37/300 | Loss 0.062849 | 154.18 (s)\n",
      "tr @ epoch 38/300 | Loss 0.059812 | 154.11 (s)\n",
      "tr @ epoch 39/300 | Loss 0.060492 | 154.13 (s)\n",
      "tr @ epoch 40/300 | Loss 0.060141 | 154.16 (s)\n",
      "tr @ epoch 41/300 | Loss 0.058745 | 154.15 (s)\n",
      "tr @ epoch 42/300 | Loss 0.057010 | 154.11 (s)\n",
      "tr @ epoch 43/300 | Loss 0.057089 | 154.17 (s)\n",
      "tr @ epoch 44/300 | Loss 0.057662 | 154.11 (s)\n",
      "tr @ epoch 45/300 | Loss 0.056163 | 153.92 (s)\n",
      "tr @ epoch 46/300 | Loss 0.057087 | 153.74 (s)\n",
      "tr @ epoch 47/300 | Loss 0.054594 | 153.70 (s)\n",
      "tr @ epoch 48/300 | Loss 0.055730 | 153.76 (s)\n",
      "tr @ epoch 49/300 | Loss 0.053365 | 153.78 (s)\n",
      "tr @ epoch 50/300 | Loss 0.056104 | 153.84 (s)\n",
      "tr @ epoch 51/300 | Loss 0.051549 | 153.79 (s)\n",
      "tr @ epoch 52/300 | Loss 0.051159 | 153.77 (s)\n",
      "tr @ epoch 53/300 | Loss 0.050697 | 153.75 (s)\n",
      "tr @ epoch 54/300 | Loss 0.048453 | 153.77 (s)\n",
      "tr @ epoch 55/300 | Loss 0.050691 | 153.62 (s)\n",
      "tr @ epoch 56/300 | Loss 0.049354 | 153.59 (s)\n",
      "tr @ epoch 57/300 | Loss 0.048382 | 153.61 (s)\n",
      "tr @ epoch 58/300 | Loss 0.049679 | 153.70 (s)\n",
      "tr @ epoch 59/300 | Loss 0.053136 | 153.61 (s)\n",
      "tr @ epoch 60/300 | Loss 0.049881 | 153.61 (s)\n",
      "tr @ epoch 61/300 | Loss 0.050025 | 153.72 (s)\n",
      "tr @ epoch 62/300 | Loss 0.051196 | 153.72 (s)\n",
      "tr @ epoch 63/300 | Loss 0.046169 | 153.62 (s)\n",
      "tr @ epoch 64/300 | Loss 0.047411 | 153.64 (s)\n",
      "tr @ epoch 65/300 | Loss 0.048994 | 153.80 (s)\n",
      "tr @ epoch 66/300 | Loss 0.046581 | 153.68 (s)\n",
      "tr @ epoch 67/300 | Loss 0.047902 | 153.60 (s)\n",
      "tr @ epoch 68/300 | Loss 0.045478 | 153.60 (s)\n",
      "tr @ epoch 69/300 | Loss 0.046935 | 153.69 (s)\n",
      "tr @ epoch 70/300 | Loss 0.049457 | 153.64 (s)\n",
      "tr @ epoch 71/300 | Loss 0.046827 | 153.58 (s)\n",
      "tr @ epoch 72/300 | Loss 0.048164 | 153.59 (s)\n",
      "tr @ epoch 73/300 | Loss 0.048274 | 153.64 (s)\n",
      "tr @ epoch 74/300 | Loss 0.045155 | 153.66 (s)\n",
      "tr @ epoch 75/300 | Loss 0.046294 | 153.64 (s)\n",
      "tr @ epoch 76/300 | Loss 0.043904 | 153.65 (s)\n",
      "tr @ epoch 77/300 | Loss 0.043428 | 153.65 (s)\n",
      "tr @ epoch 78/300 | Loss 0.042268 | 153.80 (s)\n",
      "tr @ epoch 79/300 | Loss 0.043786 | 153.66 (s)\n",
      "tr @ epoch 80/300 | Loss 0.042415 | 153.57 (s)\n",
      "tr @ epoch 81/300 | Loss 0.041802 | 153.58 (s)\n",
      "tr @ epoch 82/300 | Loss 0.044146 | 153.71 (s)\n",
      "tr @ epoch 83/300 | Loss 0.042756 | 153.73 (s)\n",
      "tr @ epoch 84/300 | Loss 0.042394 | 153.67 (s)\n",
      "tr @ epoch 85/300 | Loss 0.041619 | 153.60 (s)\n",
      "tr @ epoch 86/300 | Loss 0.041080 | 153.59 (s)\n",
      "tr @ epoch 87/300 | Loss 0.042698 | 153.71 (s)\n",
      "tr @ epoch 88/300 | Loss 0.042759 | 153.65 (s)\n",
      "tr @ epoch 89/300 | Loss 0.043280 | 153.50 (s)\n",
      "tr @ epoch 90/300 | Loss 0.043880 | 153.53 (s)\n",
      "tr @ epoch 91/300 | Loss 0.042077 | 153.65 (s)\n",
      "tr @ epoch 92/300 | Loss 0.040745 | 153.67 (s)\n",
      "tr @ epoch 93/300 | Loss 0.040330 | 153.58 (s)\n",
      "tr @ epoch 94/300 | Loss 0.042285 | 153.56 (s)\n",
      "tr @ epoch 95/300 | Loss 0.040362 | 153.66 (s)\n",
      "tr @ epoch 96/300 | Loss 0.045081 | 153.69 (s)\n",
      "tr @ epoch 97/300 | Loss 0.040961 | 153.75 (s)\n",
      "tr @ epoch 98/300 | Loss 0.040164 | 153.75 (s)\n",
      "tr @ epoch 99/300 | Loss 0.042238 | 153.72 (s)\n",
      "tr @ epoch 100/300 | Loss 0.039973 | 153.67 (s)\n",
      "tr @ epoch 101/300 | Loss 0.038179 | 153.70 (s)\n",
      "tr @ epoch 102/300 | Loss 0.038311 | 153.71 (s)\n",
      "tr @ epoch 103/300 | Loss 0.038756 | 153.65 (s)\n",
      "tr @ epoch 104/300 | Loss 0.038684 | 153.61 (s)\n",
      "tr @ epoch 105/300 | Loss 0.039729 | 153.58 (s)\n",
      "tr @ epoch 106/300 | Loss 0.038656 | 153.65 (s)\n",
      "tr @ epoch 107/300 | Loss 0.039018 | 153.66 (s)\n",
      "tr @ epoch 108/300 | Loss 0.037214 | 153.58 (s)\n",
      "tr @ epoch 109/300 | Loss 0.038232 | 153.68 (s)\n",
      "tr @ epoch 110/300 | Loss 0.038845 | 153.72 (s)\n",
      "tr @ epoch 111/300 | Loss 0.039031 | 153.68 (s)\n",
      "tr @ epoch 112/300 | Loss 0.038181 | 153.62 (s)\n",
      "tr @ epoch 113/300 | Loss 0.037666 | 153.55 (s)\n",
      "tr @ epoch 114/300 | Loss 0.038346 | 153.55 (s)\n",
      "tr @ epoch 115/300 | Loss 0.037474 | 153.59 (s)\n",
      "tr @ epoch 116/300 | Loss 0.038277 | 153.68 (s)\n",
      "tr @ epoch 117/300 | Loss 0.037645 | 153.70 (s)\n",
      "tr @ epoch 118/300 | Loss 0.038483 | 153.61 (s)\n",
      "tr @ epoch 119/300 | Loss 0.038294 | 153.59 (s)\n",
      "tr @ epoch 120/300 | Loss 0.036886 | 153.69 (s)\n",
      "tr @ epoch 121/300 | Loss 0.038631 | 153.67 (s)\n",
      "tr @ epoch 122/300 | Loss 0.037136 | 153.66 (s)\n",
      "tr @ epoch 123/300 | Loss 0.037392 | 153.56 (s)\n",
      "tr @ epoch 124/300 | Loss 0.038066 | 153.61 (s)\n",
      "tr @ epoch 125/300 | Loss 0.036835 | 153.71 (s)\n",
      "tr @ epoch 126/300 | Loss 0.034028 | 153.82 (s)\n",
      "tr @ epoch 127/300 | Loss 0.035478 | 153.65 (s)\n",
      "tr @ epoch 128/300 | Loss 0.036126 | 153.46 (s)\n",
      "tr @ epoch 129/300 | Loss 0.035683 | 153.60 (s)\n",
      "tr @ epoch 130/300 | Loss 0.033982 | 153.72 (s)\n",
      "tr @ epoch 131/300 | Loss 0.035223 | 153.68 (s)\n",
      "tr @ epoch 132/300 | Loss 0.035365 | 153.65 (s)\n",
      "tr @ epoch 133/300 | Loss 0.035335 | 153.66 (s)\n",
      "tr @ epoch 134/300 | Loss 0.035467 | 153.57 (s)\n",
      "tr @ epoch 135/300 | Loss 0.034890 | 153.57 (s)\n",
      "tr @ epoch 136/300 | Loss 0.033811 | 153.63 (s)\n",
      "tr @ epoch 137/300 | Loss 0.036248 | 153.73 (s)\n",
      "tr @ epoch 138/300 | Loss 0.035224 | 153.71 (s)\n",
      "tr @ epoch 139/300 | Loss 0.035652 | 153.66 (s)\n",
      "tr @ epoch 140/300 | Loss 0.035185 | 153.61 (s)\n",
      "tr @ epoch 141/300 | Loss 0.033976 | 153.63 (s)\n",
      "tr @ epoch 142/300 | Loss 0.036779 | 153.66 (s)\n",
      "tr @ epoch 143/300 | Loss 0.033695 | 153.71 (s)\n",
      "tr @ epoch 144/300 | Loss 0.034814 | 153.69 (s)\n",
      "tr @ epoch 145/300 | Loss 0.034381 | 153.52 (s)\n",
      "tr @ epoch 146/300 | Loss 0.036780 | 153.56 (s)\n",
      "tr @ epoch 147/300 | Loss 0.034124 | 153.58 (s)\n",
      "tr @ epoch 148/300 | Loss 0.033733 | 153.59 (s)\n",
      "tr @ epoch 149/300 | Loss 0.034054 | 153.60 (s)\n",
      "tr @ epoch 150/300 | Loss 0.035558 | 153.53 (s)\n",
      "tr @ epoch 151/300 | Loss 0.032237 | 153.59 (s)\n",
      "tr @ epoch 152/300 | Loss 0.032930 | 153.65 (s)\n",
      "tr @ epoch 153/300 | Loss 0.032112 | 153.61 (s)\n",
      "tr @ epoch 154/300 | Loss 0.031066 | 153.58 (s)\n",
      "tr @ epoch 155/300 | Loss 0.032939 | 153.64 (s)\n",
      "tr @ epoch 156/300 | Loss 0.032415 | 153.57 (s)\n",
      "tr @ epoch 157/300 | Loss 0.034265 | 153.59 (s)\n",
      "tr @ epoch 158/300 | Loss 0.032763 | 153.56 (s)\n",
      "tr @ epoch 159/300 | Loss 0.033884 | 153.59 (s)\n",
      "tr @ epoch 160/300 | Loss 0.033636 | 153.63 (s)\n",
      "tr @ epoch 161/300 | Loss 0.032940 | 153.63 (s)\n",
      "tr @ epoch 162/300 | Loss 0.034498 | 153.69 (s)\n",
      "tr @ epoch 163/300 | Loss 0.034444 | 153.68 (s)\n",
      "tr @ epoch 164/300 | Loss 0.032894 | 153.62 (s)\n",
      "tr @ epoch 165/300 | Loss 0.033665 | 153.60 (s)\n",
      "tr @ epoch 166/300 | Loss 0.032806 | 153.67 (s)\n",
      "tr @ epoch 167/300 | Loss 0.033734 | 153.70 (s)\n",
      "tr @ epoch 168/300 | Loss 0.030696 | 153.68 (s)\n",
      "tr @ epoch 169/300 | Loss 0.032457 | 153.71 (s)\n",
      "tr @ epoch 170/300 | Loss 0.032914 | 153.71 (s)\n",
      "tr @ epoch 171/300 | Loss 0.031641 | 153.66 (s)\n",
      "tr @ epoch 172/300 | Loss 0.032669 | 153.61 (s)\n",
      "tr @ epoch 173/300 | Loss 0.032839 | 153.66 (s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tr @ epoch 174/300 | Loss 0.032361 | 153.67 (s)\n",
      "tr @ epoch 175/300 | Loss 0.033135 | 153.67 (s)\n",
      "tr @ epoch 176/300 | Loss 0.030696 | 153.62 (s)\n",
      "tr @ epoch 177/300 | Loss 0.030979 | 153.60 (s)\n",
      "tr @ epoch 178/300 | Loss 0.029967 | 153.63 (s)\n",
      "tr @ epoch 179/300 | Loss 0.031005 | 153.66 (s)\n",
      "tr @ epoch 180/300 | Loss 0.030272 | 153.61 (s)\n",
      "tr @ epoch 181/300 | Loss 0.031102 | 153.65 (s)\n",
      "tr @ epoch 182/300 | Loss 0.032430 | 153.89 (s)\n",
      "tr @ epoch 183/300 | Loss 0.029687 | 154.14 (s)\n",
      "tr @ epoch 184/300 | Loss 0.031529 | 154.21 (s)\n",
      "tr @ epoch 185/300 | Loss 0.031119 | 154.14 (s)\n",
      "tr @ epoch 186/300 | Loss 0.032615 | 154.09 (s)\n",
      "tr @ epoch 187/300 | Loss 0.030944 | 154.10 (s)\n",
      "tr @ epoch 188/300 | Loss 0.031199 | 154.06 (s)\n",
      "tr @ epoch 189/300 | Loss 0.030461 | 154.06 (s)\n",
      "tr @ epoch 190/300 | Loss 0.032551 | 153.95 (s)\n",
      "tr @ epoch 191/300 | Loss 0.031372 | 154.00 (s)\n",
      "tr @ epoch 192/300 | Loss 0.030019 | 153.98 (s)\n",
      "tr @ epoch 193/300 | Loss 0.031050 | 154.01 (s)\n",
      "tr @ epoch 194/300 | Loss 0.032434 | 154.02 (s)\n",
      "tr @ epoch 195/300 | Loss 0.030898 | 154.03 (s)\n",
      "tr @ epoch 196/300 | Loss 0.030056 | 154.06 (s)\n",
      "tr @ epoch 197/300 | Loss 0.032221 | 154.06 (s)\n",
      "tr @ epoch 198/300 | Loss 0.031181 | 153.84 (s)\n",
      "tr @ epoch 199/300 | Loss 0.031069 | 153.63 (s)\n",
      "tr @ epoch 200/300 | Loss 0.031572 | 153.52 (s)\n",
      "tr @ epoch 201/300 | Loss 0.028604 | 153.45 (s)\n",
      "tr @ epoch 202/300 | Loss 0.028391 | 153.53 (s)\n",
      "tr @ epoch 203/300 | Loss 0.030516 | 153.55 (s)\n",
      "tr @ epoch 204/300 | Loss 0.029093 | 153.53 (s)\n",
      "tr @ epoch 205/300 | Loss 0.029848 | 153.45 (s)\n",
      "tr @ epoch 206/300 | Loss 0.029157 | 153.53 (s)\n",
      "tr @ epoch 207/300 | Loss 0.029933 | 153.53 (s)\n",
      "tr @ epoch 208/300 | Loss 0.029888 | 153.61 (s)\n",
      "tr @ epoch 209/300 | Loss 0.029262 | 153.56 (s)\n",
      "tr @ epoch 210/300 | Loss 0.030348 | 153.53 (s)\n",
      "tr @ epoch 211/300 | Loss 0.029423 | 153.65 (s)\n",
      "tr @ epoch 212/300 | Loss 0.028832 | 153.56 (s)\n",
      "tr @ epoch 213/300 | Loss 0.027666 | 153.45 (s)\n",
      "tr @ epoch 214/300 | Loss 0.029864 | 153.50 (s)\n",
      "tr @ epoch 215/300 | Loss 0.029318 | 153.55 (s)\n",
      "tr @ epoch 216/300 | Loss 0.027845 | 153.53 (s)\n",
      "tr @ epoch 217/300 | Loss 0.030493 | 153.45 (s)\n",
      "tr @ epoch 218/300 | Loss 0.028439 | 153.48 (s)\n",
      "tr @ epoch 219/300 | Loss 0.029591 | 153.35 (s)\n",
      "tr @ epoch 220/300 | Loss 0.030057 | 153.33 (s)\n",
      "tr @ epoch 221/300 | Loss 0.030739 | 153.41 (s)\n",
      "tr @ epoch 222/300 | Loss 0.029040 | 153.44 (s)\n",
      "tr @ epoch 223/300 | Loss 0.029478 | 153.43 (s)\n",
      "tr @ epoch 224/300 | Loss 0.029659 | 153.51 (s)\n",
      "tr @ epoch 225/300 | Loss 0.028828 | 153.55 (s)\n",
      "tr @ epoch 226/300 | Loss 0.028114 | 153.57 (s)\n",
      "tr @ epoch 227/300 | Loss 0.027806 | 153.51 (s)\n",
      "tr @ epoch 228/300 | Loss 0.028897 | 153.49 (s)\n",
      "tr @ epoch 229/300 | Loss 0.028635 | 153.55 (s)\n",
      "tr @ epoch 230/300 | Loss 0.029890 | 153.53 (s)\n",
      "tr @ epoch 231/300 | Loss 0.028519 | 153.51 (s)\n",
      "tr @ epoch 232/300 | Loss 0.028312 | 153.52 (s)\n",
      "tr @ epoch 233/300 | Loss 0.027789 | 153.55 (s)\n",
      "tr @ epoch 234/300 | Loss 0.028541 | 153.61 (s)\n",
      "tr @ epoch 235/300 | Loss 0.029069 | 153.60 (s)\n",
      "tr @ epoch 236/300 | Loss 0.028948 | 153.59 (s)\n",
      "tr @ epoch 237/300 | Loss 0.028406 | 153.51 (s)\n",
      "tr @ epoch 238/300 | Loss 0.026953 | 153.50 (s)\n",
      "tr @ epoch 239/300 | Loss 0.027727 | 153.55 (s)\n",
      "tr @ epoch 240/300 | Loss 0.028731 | 153.48 (s)\n",
      "tr @ epoch 241/300 | Loss 0.027825 | 153.46 (s)\n",
      "tr @ epoch 242/300 | Loss 0.026921 | 153.57 (s)\n",
      "tr @ epoch 243/300 | Loss 0.028080 | 153.60 (s)\n",
      "tr @ epoch 244/300 | Loss 0.029763 | 153.52 (s)\n",
      "tr @ epoch 245/300 | Loss 0.027232 | 153.67 (s)\n",
      "tr @ epoch 246/300 | Loss 0.029164 | 153.85 (s)\n",
      "tr @ epoch 247/300 | Loss 0.028266 | 153.81 (s)\n",
      "tr @ epoch 248/300 | Loss 0.029505 | 153.66 (s)\n",
      "tr @ epoch 249/300 | Loss 0.027816 | 153.55 (s)\n",
      "tr @ epoch 250/300 | Loss 0.028246 | 153.66 (s)\n",
      "tr @ epoch 251/300 | Loss 0.027193 | 153.63 (s)\n",
      "tr @ epoch 252/300 | Loss 0.028112 | 153.60 (s)\n",
      "tr @ epoch 253/300 | Loss 0.027194 | 153.57 (s)\n",
      "tr @ epoch 254/300 | Loss 0.027346 | 153.54 (s)\n",
      "tr @ epoch 255/300 | Loss 0.027579 | 153.56 (s)\n",
      "tr @ epoch 256/300 | Loss 0.027416 | 153.57 (s)\n",
      "tr @ epoch 257/300 | Loss 0.027582 | 153.51 (s)\n",
      "tr @ epoch 258/300 | Loss 0.026860 | 153.61 (s)\n",
      "tr @ epoch 259/300 | Loss 0.027161 | 153.68 (s)\n",
      "tr @ epoch 260/300 | Loss 0.027414 | 153.76 (s)\n",
      "tr @ epoch 261/300 | Loss 0.026729 | 153.72 (s)\n",
      "tr @ epoch 262/300 | Loss 0.027009 | 153.69 (s)\n",
      "tr @ epoch 263/300 | Loss 0.027401 | 153.62 (s)\n",
      "tr @ epoch 264/300 | Loss 0.027289 | 153.48 (s)\n",
      "tr @ epoch 265/300 | Loss 0.028223 | 153.59 (s)\n",
      "tr @ epoch 266/300 | Loss 0.027299 | 153.71 (s)\n",
      "tr @ epoch 267/300 | Loss 0.027454 | 153.69 (s)\n",
      "tr @ epoch 268/300 | Loss 0.028229 | 153.93 (s)\n",
      "tr @ epoch 269/300 | Loss 0.026530 | 153.83 (s)\n",
      "tr @ epoch 270/300 | Loss 0.026646 | 153.57 (s)\n",
      "tr @ epoch 271/300 | Loss 0.027156 | 153.44 (s)\n",
      "tr @ epoch 272/300 | Loss 0.027028 | 153.51 (s)\n",
      "tr @ epoch 273/300 | Loss 0.026458 | 153.50 (s)\n",
      "tr @ epoch 274/300 | Loss 0.026924 | 153.45 (s)\n",
      "tr @ epoch 275/300 | Loss 0.027985 | 153.50 (s)\n",
      "tr @ epoch 276/300 | Loss 0.026761 | 153.53 (s)\n",
      "tr @ epoch 277/300 | Loss 0.026262 | 153.58 (s)\n",
      "tr @ epoch 278/300 | Loss 0.026816 | 153.49 (s)\n",
      "tr @ epoch 279/300 | Loss 0.026666 | 153.45 (s)\n",
      "tr @ epoch 280/300 | Loss 0.026568 | 153.49 (s)\n",
      "tr @ epoch 281/300 | Loss 0.026613 | 153.52 (s)\n",
      "tr @ epoch 282/300 | Loss 0.026090 | 153.62 (s)\n",
      "tr @ epoch 283/300 | Loss 0.026129 | 153.53 (s)\n",
      "tr @ epoch 284/300 | Loss 0.026435 | 153.49 (s)\n",
      "tr @ epoch 285/300 | Loss 0.026812 | 153.42 (s)\n",
      "tr @ epoch 286/300 | Loss 0.026304 | 153.48 (s)\n",
      "tr @ epoch 287/300 | Loss 0.026987 | 153.47 (s)\n",
      "tr @ epoch 288/300 | Loss 0.026557 | 153.49 (s)\n",
      "tr @ epoch 289/300 | Loss 0.027287 | 153.53 (s)\n",
      "tr @ epoch 290/300 | Loss 0.026491 | 153.50 (s)\n",
      "tr @ epoch 291/300 | Loss 0.025690 | 153.45 (s)\n",
      "tr @ epoch 292/300 | Loss 0.027550 | 153.54 (s)\n",
      "tr @ epoch 293/300 | Loss 0.027162 | 153.61 (s)\n",
      "tr @ epoch 294/300 | Loss 0.026158 | 153.62 (s)\n",
      "tr @ epoch 295/300 | Loss 0.026080 | 153.59 (s)\n",
      "tr @ epoch 296/300 | Loss 0.026893 | 153.47 (s)\n",
      "tr @ epoch 297/300 | Loss 0.026055 | 153.50 (s)\n",
      "tr @ epoch 298/300 | Loss 0.027319 | 153.53 (s)\n",
      "tr @ epoch 299/300 | Loss 0.026677 | 153.54 (s)\n",
      "tr @ epoch 300/300 | Loss 0.027238 | 153.53 (s)\n"
     ]
    }
   ],
   "source": [
    "fmot.train(loader_tr, optimizer, epochs=epochs, scheduler=scheduler, eval_int=int(0), save_int=int(300), generate=False, save_path=spath,saved_model=saved_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92510680",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b295ce55",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "model_path = os.path.join(spath, 'epoch_300.pt')\n",
    "checkpoint = torch.load(model_path, map_location='cpu', weights_only=False)\n",
    "model.load_state_dict(checkpoint, strict=False)\n",
    "fmot = OFMModel(model, kernel_length=kernel_length, kernel_variance=kernel_variance, nu=nu, sigma_min=sigma_min, device=device, x_dim=x_dim, n_pos=n_pos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad9bf6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_meta_info(batch_size, query_dims, n_pos=n_pos):\n",
    "\n",
    "    pos_data = n_pos.unsqueeze(0).repeat(batch_size, 1, 1)\n",
    "    \n",
    "    query_n_pos = make_2d_grid(query_dims)\n",
    "    query_pos_data = query_n_pos.unsqueeze(0).repeat(batch_size, 1, 1)\n",
    "\n",
    "    collated_batch = {}\n",
    "\n",
    "\n",
    "    collated_batch[\"input_pos\"] = pos_data.permute(0, 2, 1)\n",
    "    collated_batch['query_pos']= query_pos_data.permute(0, 2, 1)\n",
    "\n",
    "    return collated_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c78fae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4847b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "\n",
    "    X_alt = []\n",
    "    for i in range(25):\n",
    "        collated_batch =  gen_meta_info(batch_size=100, n_pos=n_pos, query_dims=query_dims)\n",
    "        pos, query_pos = collated_batch['input_pos'], collated_batch['query_pos']\n",
    "        X_temp = fmot.sample(pos=pos.to(device), query_pos=query_pos.to(device), n_samples=200, n_channels=3, n_eval=2).cpu()\n",
    "    \n",
    "        #X_temp = fmot.sample(pos=pos_data[:200].to(device), n_samples=200, n_eval=10).cpu()\n",
    "        X_alt.append(X_temp)\n",
    "        \n",
    "    X_alt = torch.vstack(X_alt).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c255540",
   "metadata": {},
   "outputs": [],
   "source": [
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7f4b26",
   "metadata": {},
   "source": [
    "## Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a281a50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test =  np.load('../dataset/cylinder/x_test.npy')\n",
    "x_test = torch.Tensor(x_test[:5000]).permute(0,2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defd3dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "swd_value = swd_stable(X=X_alt, Y=x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24092401",
   "metadata": {},
   "outputs": [],
   "source": [
    "mmd_value = unbiased_mmd2_torch(X=X_alt, Y=x_test, device=device)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
